%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Sample document for preparing papers to  "Avtomatika i Telemekhanika"
%%  charset=utf-8
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{a&t}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{url}
\usepackage{multirow}

\usepackage{autonum}

\begin{document}  %%%!!!

\year{2020}
\title{Analysis of privilege learning and distillation models.}%
\thanks{This research was supported by \dots}

\authors{A.V.~GRABOVOY\\
(Moscow Institute of Physics and Technology, Moscow)\\
V.V.~STRIJOV, Doctor of Science in physics and mathematics\\
(A.A. Dorodnicyn Computing Center, Federal Research Center ``Computer Science and Control'' of the Russian Academy of Sciences)}

\maketitle

\begin{abstract}
This paper analyses methods for reducing the complexity of approximating models. It proposes a probabilistic approach to distillation and privilege learning methods. The article presents an analysis of the parametric function with a fixed structure. The paper shows a theoretical foundation for the linear and logistic regression. The computational experiment illustrates the theoretical results with synthetic datasets and real datasets. It uses FashionMNIST and Twitter Sentiment Analysis datasets as real data.

\smallskip\\
\textit{Key words}: model selection; bayesian inference; distillation; privilege learning.
\end{abstract}

\AdditionalInformation{Grabovoy A.V.}{Moscow Institute of Physics and Technology, student, Moscow}{grabovoy.av@phystech.edu}

\AdditionalInformation{Strijov V.V.}{A.A. Dorodnicyn Computing Center, Federal Research Center ``Computer Science and Control'' of the Russian Academy of Sciences, leading scientist, Moscow}{strijov@phystech.edu}


\end{document}
