\documentclass[12pt, twoside]{article}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}

\usepackage{amsthm}
\usepackage{a4wide}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{euscript}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{caption}
\usepackage{color}
\usepackage{bm}
\usepackage{tabularx}
\usepackage{adjustbox}


\usepackage[toc,page]{appendix}

\usepackage{comment}
\usepackage{rotating}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]

\numberwithin{equation}{section}

\newcommand*{\No}{No.}

\usepackage{autonum}

\begin{document}

\title{\bf Privilege Learning \thanks{Работа выполнена при поддержке РФФИ и правительства РФ.}}
\date{}
\author{}
\maketitle

\begin{center}
\bf
А.\,В.~Грабовой\footnote{афиляция автора}, В.\,В.~Стрижов\footnote{афиляция автора}

\end{center}

{\centering\begin{quote}
\textbf{Аннотация:} 
\smallskip
\textbf{Ключевые слова}: смесь экспертов; байесовский выбор модели; априорное распределение.

\smallskip
\textbf{DOI}: 00.00000/00000000000000
\end{quote}
}

\section{Введение}
\section{Постановка задачи}
Пусть задано множество объектов~$\bm{\Omega}$ и пространство целевых переменных~$\mathbb{Y}$:
\[
\label{eq:st:1}
\begin{aligned}
\bm{\Omega}, \quad \left|\bm{\Omega}\right| = N,
\end{aligned}
\]
где~$N$~--- число объектов, множество~$\mathbb{Y}=\{1,\cdots,K\}$ для задачи классификации, где~$K$ число классов, множество $\mathbb{Y}=\mathbb{R}$ для задачи регрессии.
Для множества~$\bm{\Omega}$ задано отображение в некоторое признаковое пространство~$\mathbb{R}^{n}$:
\[
\label{eq:st:phi}
\begin{aligned}
\varphi:\bm{\Omega} \to \mathbb{R}^{n},
\end{aligned}
\]
где~$n$ размерность признакового пространства. Обозначим~$\varphi(\bm{\Omega}) = \textbf{X}$.
Пусть для некоторых объектов~$\bm{\Omega}^* \subset \bm{\Omega}$ задана привилегированная информация:
\[
\label{eq:st:phi*}
\begin{aligned}
\varphi^*:\bm{\Omega}^* \to \mathbb{R}^{n^*}, \quad \left|\bm{\Omega}^*\right| = N^*,
\end{aligned}
\]
где~$N^* \leq N$~--- число объектов с привилегированной информацией, $n^*$~--- число признаков в пространстве привилегированной информации. Обозначим~$\varphi^*(\bm{\Omega}^*) = \textbf{X}^*$.

Множество индексов объектов для которых известна привилегированная информация обозначим~$\mathcal{I}$:
\[
\label{eq:st:3}
\begin{aligned}
\mathcal{I} = \{1 \leq i \leq N |~\text{для $i$-го объекта задана привилегированная информация}\}.
\end{aligned}
\]

Пусть на множестве привилегированных признаков задана некоторая функция (или эксперт)~$\textbf{f}\bigr(\textbf{x}^*\bigr)$:
\[
\label{eq:st:4}
\begin{aligned}
\textbf{f}:\mathbb{R}^{n^*} \to \mathbb{Y}^*,
\end{aligned}
\]
где~$\mathbb{Y}^*=\mathbb{Y}$ для задачи регрессии и $\mathbb{Y}^*$ является единичным симплексом в пространстве размерности~$K$ (содержит вектора вероятностей каждого класса). Обозначим ответы модели~$\textbf{f}\bigr(\textbf{x}_i\bigr)=s_i$.  Получим вектор ответом~$\textbf{s}$ модели учителя~$\textbf{f}$.

Требуется построить модель~$\textbf{g}\bigr(\textbf{x}\bigr)$ над множеством исходных признаков:
\[
\label{eq:st:5}
\begin{aligned}
\textbf{g}:\mathbb{R}^{n} \to \mathbb{Y}.
\end{aligned}
\]
\subsection{Без привилегированной информации}
Пусть~$\textbf{g}$ выбирается из некоторого семейства:
\[
\label{eq:st:G}
\begin{aligned}
\mathcal{G} = \left\{\textbf{g}| \textbf{g}:\mathbb{R}^{n} \to \mathbb{Y}.\right\}
\end{aligned}
\]
Для поиска~$\hat{\textbf{g}}$ воспользуемся методом максимального правдоподобия:
\[
\label{eq:st:6}
\begin{aligned}
p\bigr(\textbf{y}|\textbf{X}, \textbf{g}\bigr) = \prod_{i=1}^{N}p\bigr(y_{i}|\textbf{x}_i, \textbf{g}\bigr).
\end{aligned}
\]
В качестве~$\hat{\textbf{g}}$ выберем то, которое максимизирует правдоподобие модели~\eqref{eq:st:6}:
\[
\label{eq:st:7}
\begin{aligned}
\hat{\textbf{g}} = \arg\max_{\textbf{g}\in \mathcal{G}} \prod_{i=1}^{N}p\bigr(y_{i}|\textbf{x}_i, \textbf{g}\bigr).
\end{aligned}
\]
\subsection{С учетом привилегированной информации}
Рассмотрим следующую вероятностную постановку, в которой должны быть выполнены следующие ограничения:
\begin{enumerate}
	\item $\forall \omega \in \bm{\Omega}^*$ элементы $y(\omega)$ и $s(\omega)$ являются зависимыми величинами,
	\item Если $|\bm{\Omega}^*|=0$ то решение должно соответствовать решению~\eqref{eq:st:7}
	\item Рассмотрим параметр~$\lambda\in[0,1]$ как уровень доверия к ответам модели~$\mathbf{f}$. Будем рассматривать его как вероятность того, что на этапе обучения модель~$\mathbf{g}$ выбирает ответ модели~$\mathbf{f}$ как более верный чем метка~$y$.
\end{enumerate}

Рассмотрим совместное правдоподобие:
\[
\label{eq:st:8}
\begin{aligned}
p\bigr(\textbf{y}, \textbf{s}|\textbf{X}, \textbf{g}, \mathcal{I}, \lambda\bigr)=\prod_{i\not\in \mathcal{I}}p\bigr(y_i|\textbf{X}, \textbf{g}\bigr)\prod_{i\in \mathcal{I}}p\bigr(y_i, s_i|\textbf{X}, \textbf{g}, \lambda\bigr).
\end{aligned}
\]
Расписав~$p\bigr(y_i, s_i|\textbf{X}, \textbf{g}, \lambda\bigr)$, получаем:
\[
\label{eq:st:9}
\begin{aligned}
p\bigr(\textbf{y}, \textbf{s}|\textbf{X}, \textbf{g}, \mathcal{I}, \lambda\bigr)=\prod_{i\not\in \mathcal{I}}p\bigr(y_i|\textbf{X}, \textbf{g}\bigr)\prod_{i\in \mathcal{I}}p^{1-\lambda}\bigr(y_i|s_i, \textbf{X}, \textbf{g}\bigr)\prod_{i\in \mathcal{I}}p^{\lambda}\bigr(s_i|\textbf{X}, \textbf{g}\bigr).
\end{aligned}
\]
Пусть~$y_i$ и~$s_i$ независимы, тогда получаем совместное правдоподобие:
\[
\label{eq:st:10}
\begin{aligned}
p\bigr(\textbf{y}, \textbf{s}|\textbf{X}, \textbf{g}, \mathcal{I}, \lambda\bigr)=\prod_{i\not\in \mathcal{I}}p\bigr(y_i|\textbf{X}, \textbf{g}\bigr)\prod_{i\in \mathcal{I}}p^{1-\lambda}\bigr(y_i|\textbf{X}, \textbf{g}\bigr)\prod_{i\in \mathcal{I}}p^{\lambda}\bigr(s_i|\textbf{X}, \textbf{g}\bigr).
\end{aligned}
\]
Используя~\eqref{eq:st:10} получаем следующую оптимизационную задачу для поиска~$\hat{\textbf{g}}$
\[
\label{eq:st:11}
\begin{aligned}
\hat{\textbf{g}} = \arg\max_{\textbf{g}\in \mathcal{G}} \prod_{i\not\in \mathcal{I}}p\bigr(y_i|\textbf{X}, \textbf{g}\bigr)\prod_{i\in \mathcal{I}}p^{1-\lambda}\bigr(y_i|\textbf{X}, \textbf{g}\bigr)\prod_{i\in \mathcal{I}}p^{\lambda}\bigr(s_i|\textbf{X}, \textbf{g}\bigr).
\end{aligned}
\]
\subsection{Случай регрессии}
Рассмотрим следующие распределения:
\[
\label{eq:st:12}
\begin{aligned}
p\bigr(y_i|\textbf{x}_i, \textbf{g}\bigr) &= N\bigr(y_i|\textbf{g}(\textbf{x}), \bm{\Sigma}\bigr), \\
p\bigr(s_i|\textbf{x}_i, \textbf{g}\bigr) &= N\bigr(s_i|\textbf{g}(\textbf{x}),  \bm{\Sigma}\bigr), \\
\end{aligned}
\]
где~$\bm{\Sigma} = \textbf{I}$.
Подставляя~\eqref{eq:st:12} в~\eqref{eq:st:11} и прологарифмировав, получим:
\[
\label{eq:st:13}
\begin{aligned}
\hat{\textbf{g}} = \arg\min_{\textbf{g}\in \mathcal{G}}\sum_{i\not\in \mathcal{I}}\left(y_i-\textbf{g}(\textbf{x})\right)^{2}+\left(1-\lambda\right)\sum_{i\in \mathcal{I}}\left(y_i-\textbf{g}(\textbf{x})\right)^{2}+\lambda\sum_{i\in \mathcal{I}}\left(s_i-\textbf{g}(\textbf{x})\right)^{2}.
\end{aligned}
\]


\begin{thebibliography}{99}
\bibitem{Vapnik2015}
	\textit{Vladimir Vapnik, Rauf Izmailov} Learning Using Privileged Information: Similarity Control and Knowledge Transfer // Journal of Machine Learning Research. 2015. No 16. pp. 2023--2049.
\bibitem{Lopez2016}
	\textit{David Lopez-Paz, Leon Bottou, Bernhard Scholkopf, Vladimir Vapnik} UNIFYING DISTILLATION
AND PRIVILEGED INFORMATION // Published as a conference paper at ICLR. 2016.
 \end{thebibliography}


\end{document}

